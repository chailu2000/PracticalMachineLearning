anova(fit2, fit3)
lm(mpg ~ I(wt * 0.5) + factor(cyl), data = mtcars)
x <- c(0.586, 0.166, -0.042, -0.614, 11.72)
y <- c(0.549, -0.026, -0.127, -0.751, 1.344)
lm.influence(lm(y~x), do.coef = TRUE)
plot(hat(model.matrix(lm(y~x))), type = "h")
x <- c(0.586, 0.166, -0.042, -0.614, 11.72)
y <- c(0.549, -0.026, -0.127, -0.751, 1.344)
dfbetas(lm(y~x))
lm.influence(lm(y~x), do.coef = TRUE)
plot(hat(model.matrix(lm(y~x))), type = "h")
plot(hat(model.matrix(lm(y~x))), type = "h")
plot(hat(model.matrix(lm(y~x))), type = "h")
fit <- lm ( y~x)
plot(hat(model.matrix(fit), type = "h")
plot(hat(model.matrix(fit)), type = "h")
plot(hat(model.matrix(fit), type = "h"))
plot(hat(model.matrix(fit), type = "h")
plot(hat(model.matrix(fit)), type = "h")
fit <- lm (y~x)
fit <- lm(y~x)
plot(hat(model.matrix(fit)), type = "h")
lm.influence(fit, do.coef = TRUE)
x <- c(0.586, 0.166, -0.042, -0.614, 11.72)
y <- c(0.549, -0.026, -0.127, -0.751, 1.344)
fit <- lm(y~x)
lm.influence(fit, do.coef = TRUE)
plot(hat(model.matrix(fit)), type = "h")
dfbetas(fit)
library(AppliedPredictiveModeling)
data(segmentationOriginal)
library(caret)
inTrain <- createDataPartition(y=Case,p=0.75, list=FALSE, data=segmentationOriginal)
inTrain <- createDataPartition(y=segmentationOriginal$Case,p=0.75, list=FALSE)
training <- segmentationOriginal[inTrain,]
testing <- segmentationOriginal[-inTrain,]
dim(training)
str(training)
seed(125)
setSeed(125)
help seed
help(seed)
??sed
??seed
set.seed(125)
modFit <- train(Class ~ .,method="rpart",data=training)
print(modFit$finalModel)
plot(modFit$finalModel, uniform=TRUE,
main="Classification Tree")
text(modFit$finalModel, use.n=TRUE, all=TRUE, cex=.8)
library(rattle)
fancyRpartPlot(modFit$finalModel)
install.packages('rattle')
library(rattle)
fancyRpartPlot(modFit$finalModel)
install.packages('rpart.plot')
library(rattle)
fancyRpartPlot(modFit$finalModel)
library(pgmm)
data(olive)
olive = olive[,-1]
install.packages('pgmm')
library(pgmm)
data(olive)
olive = olive[,-1]
library(caret)
inTrain <- createDataPartition(y=olive$Area,
p=0.7, list=FALSE)
training <- iris[inTrain,]
testing <- iris[-inTrain,]
dim(training); dim(testing)
qplot(Petal.Width,Sepal.Width,colour=Species,data=training)
qplot(Petal.Width,Sepal.Width,colour=Area,data=training)
str(training)
inTrain <- createDataPartition(y=olive$Area,
p=0.7, list=FALSE)
training <- olive[inTrain,]
testing <- olive[-inTrain,]
dim(training); dim(testing)
str(training)
modFit <- train(Area ~ .,method="rpart",data=training)
print(modFit$finalModel)
qplot(Eicosenoic,Linoleic,colour=Area,data=training)
newdata = as.data.frame(t(colMeans(olive)))
predict(modFit,newdata=newdata)
predict(modFit,newdata)
inTrain <- createDataPartition(y=olive$Area,
p=0.75, list=FALSE)
training <- olive[inTrain,]
testing <- olive[-inTrain,]
dim(training); dim(testing)
modFit <- train(Area ~ .,method="rpart",data=olive)
print(modFit$finalModel)
predict(modFit,newdata)
library(pgmm)
data(olive)
olive = olive[,-1]
str(olive)
modFit <- train(Area ~ .,method="rpart",data=olive[,-1])
print(modFit$finalModel)
olive
head(olive)
modFit <- train(olive$Area ~ .,method="rpart",data=olive[,-1])
print(modFit$finalModel)
predict(modFit,newdata)
str(olive)
tail(olive)
library(ElemStatLearn)
data(SAheart)
set.seed(8484)
train = sample(1:dim(SAheart)[1],size=dim(SAheart)[1]/2,replace=F)
trainSA = SAheart[train,]
testSA = SAheart[-train,]
install.packages('ElemStatLearn')
library(ElemStatLearn)
data(SAheart)
set.seed(8484)
train = sample(1:dim(SAheart)[1],size=dim(SAheart)[1]/2,replace=F)
trainSA = SAheart[train,]
testSA = SAheart[-train,]
str(trainSA)
modFit <- train(chd ~ age + alcohol + obesity + tobacco + typea + ldl, method="glm", family="binomial", data=trainSA)
missClass = function(values,prediction){sum(((prediction > 0.5)*1) != values)/length(values)}
print(modFit$finalModel)
prediction<-predict(modFit,newdata=testSA)
prediction
predTrain <- predict(modFit, newdata=trainSA)
predTest <- predict(modFIt, newdata=testSA)
predTest <- predict(modFit, newdata=testSA)
missClass(trainSA,predTrain)
missClass(testSA,predTest)
204.9/231
ln(204.9)
log(204.9)
set.seed(13234)
modFit <- train(chd ~ age + alcohol + obesity + tobacco + typea + ldl, method="glm", family="binomial", data=trainSA)
print(modFit$finalModel)
predTrain <- predict(modFit, newdata=trainSA)
predTest <- predict(modFit, newdata=testSA)
missClass(trainSA,predTrain)
missClass(testSA,predTest)
missClass(SAheart,predTest)
head(SAheart)
head(predTrain)
head(predTest)
204.9/423
modFit <- train(trainSA$chd ~ age + alcohol + obesity + tobacco + typea + ldl, method="glm", family="binomial", data=trainSA[,-10])
predTrain <- predict(modFit, newdata=trainSA)
predTest <- predict(modFit, newdata=testSA)
missClass(testSA,predTest)
missClass = function(values,prediction){sum(((prediction > 0.5)*1) != values)/length(values)}
missClass = function(values,prediction){sum(((prediction > 0.5)*1) != values)/length(values)}
missClass = function(values, prediction) {}
missClass = function(values,prediction){sum(((prediction > 0.5)*1) != values)/length(values)}
missClass(testSA$chd,predTest)
missClass(trainSA$chd,predTrain)
library(ElemStatLearn)
data(vowel.train)
data(vowel.test)
str(vowel.train)
vowel.train$y <- factor(vowel.train$y)
vowel.test$y <- factor(vowel.test$y)
set.seed(33833)
modFit <- train(y ~ ., method="rf", data=vowel.train)
help(varImp)
varImp(modFit)
library(AppliedPredictiveModeling)
data(segmentationOriginal)
library(caret)
set.seed(125)
library(rpart)
str(segmentationOriginal)
testQ1 <- segmentationOriginal[segmentationOriginal$CASE="Test",]
testQ1 <- segmentationOriginal[segmentationOriginal$CASE=="Test",]
trainQ1 <- segmentationOriginal[segmentationOriginal$CASE=="Train",]
fit <- rpart(Class ~ .,
method="class", data=trainQ1)
printcp(fit) # display the results
plotcp(fit) # visualize cross-validation results
summary(fit) # detailed summary of splits
fit <- rpart(Class~.,method="class",data=trainQ1)
modFit <- train(Class ~ ., method="rpart", data=trainQ1)
trainQ1
testQ1
segmentationOriginal
head(segmentationOriginal)
testQ1 <- segmentationOriginal[segmentationOriginal$CASE==Test,]
testQ1 <- segmentationOriginal[as.char(segmentationOriginal$CASE)=="Test",]
testQ1 <- segmentationOriginal[as.character(segmentationOriginal$CASE)=="Test",]
as.character(segmentationOriginal$CASE)
segmentationOriginal$CASE
str(segmentationOriginal$CASE)
str(segmentationOriginal)
testQ1 <- segmentationOriginal[segmentationOriginal$Case==Test,]
testQ1 <- segmentationOriginal[segmentationOriginal$Case=="Test",]
trainQ1 <- segmentationOriginal[segmentationOriginal$Case=="Train",]
modFit <- train(Class ~ ., method="rpart", data=trainQ1)
print(modFit$finalModel)
fancyRpartPlot(modFit$finalModel)
library(pgmm)
data(olive)
olive = olive[,-1]
head(olive)
data(olive)
head(olive)
data(olive)
olive = olive[,-1]
man(tree)
help(tree)
??tree
??tree
rpart(Area ~ ., data=olive)
rp <-rpart(Area ~ ., data=olive)
newdata = as.data.frame(t(colMeans(olive)))
predict(rp,newdata,method="class")
plot(x=factor(mtcars$am), y=mtcars$mpg)
library(ggplot2)
p <- ggplot(mtcars, aes(factor(am), mpg))
p1 <- p + geom_boxplot(outlier.shape = 3)
p1 + geom_point(position = position_jitter(width = 0.2))
summary(mtcars)
model.all <- lm(mpg ~ ., data = mtcars)
n <- nrow(mtcars)
## Stepwise Algorithm
model.new <- step(model.all, direction = "both", k = log(n), verbose=FALSE)
summary(model.new)
help(step)
model.all <- lm(mpg ~ ., data = mtcars)
n <- nrow(mtcars)
## Stepwise Algorithm
model.new <- step(model.all, direction = "both", k = log(n), trace=-1)
summary(model.new)
model.all <- lm(mpg ~ ., data = mtcars)
n <- nrow(mtcars)
## Stepwise Algorithm
model.new <- step(model.all, direction = "both", k = log(n), trace=-100)
summary(model.new)
model.all <- lm(mpg ~ ., data = mtcars)
n <- nrow(mtcars)
## Stepwise Algorithm
model.new <- step(model.all, direction = "both", k = log(n), trace=0)
summary(model.new)
model.new <- step(model.all, direction = "both", k = log(n), trace=1)
model.new <- step(model.all, direction = "both", k = log(n), trace=0)
summary(model.new)
setwd("~/coursera/REPOS/8 Practical Machine Learning")
library(caret)
preprocessData <- function(df) {
# remove columns with #DIV/0!
df <- df[,-which(names(df) %in% c("new_window","amplitude_yaw_belt", "amplitude_yaw_forearm","amplitude_yaw_dumbbell", "kurtosis_yaw_forearm", "kurtosis_yaw_belt", "skewness_yaw_belt", "kurtosis_yaw_dumbbell", "skewness_yaw_dumbbell", "skewness_yaw_forearm"))]
dropInd <- which(apply(df, 1, function(x) any(x=="#DIV/0!")))
df <- df[-dropInd,]
library(caret)
# convert factors to numeric
facs <- sapply(df[,-ncol(df)], is.factor)
df[ , facs] <- as.numeric(as.character(df[ , facs]))
# set NA to 0
df[is.na(df)] <- 0
nsv <- nearZeroVar(df,saveMetrics=TRUE)
# remove zero columns
df <- df[,-which(names(df) %in% c(rownames(nsv[nsv$zeroVar,])))]
return(df)
}
set.seed(2172015)
training <- read.csv("pml-training.csv")
testing <- read.csv("pml-testing.csv")
training<-preprocessData(training)
fitControl <- trainControl(method="repeatedcv",
number=5,
repeats=1,
verboseIter=TRUE)
gbm_model<-train(classe~.,data=training,method="gbm",
trControl=fitControl,
preProcess=c("scale", "pca"),
verbose=FALSE)
# preprocess, center, scale, pca
preProc <- preProcess(training[,-124], method="pca")
trainPC <- predict(preProc,training[,-124])
modelFit <- rpart(training$classe ~ .,method="class",data=trainPC)
testing[is.na(testing)] <- 0
testing <- testing[, which(names(testing) %in% names(training))]
testPC <- predict(preProc,testing)
test<- predict(modelFit, testPC, type="vector")
library(rpart)
modelFit <- rpart(training$classe ~ .,method="class",data=trainPC)
testing[is.na(testing)] <- 0
testing <- testing[, which(names(testing) %in% names(training))]
testPC <- predict(preProc,testing)
test<- predict(modelFit, testPC, type="vector")
test
test<- predict(modelFit, testPC)
test
test<- predict(modelFit, testPC, type="vector")
str(test)
test
testing <- read.csv("pml-testing.csv")
test<- predict(gbm_model, testing, type="vector")
test<- predict(gbm_model, testing)
testing[is.na(testing)] <- 0
testing <- testing[, which(names(testing) %in% names(training))]
test<- predict(gbm_model, testing)
test
library(caret)
preprocessData <- function(df) {
# remove columns with #DIV/0!
df <- df[,-which(names(df) %in% c("new_window","amplitude_yaw_belt", "amplitude_yaw_forearm","amplitude_yaw_dumbbell", "kurtosis_yaw_forearm", "kurtosis_yaw_belt", "skewness_yaw_belt", "kurtosis_yaw_dumbbell", "skewness_yaw_dumbbell", "skewness_yaw_forearm"))]
dropInd <- which(apply(df, 1, function(x) any(x=="#DIV/0!")))
df <- df[-dropInd,]
library(caret)
# convert factors to numeric
facs <- sapply(df[,-ncol(df)], is.factor)
df[ , facs] <- as.numeric(as.character(df[ , facs]))
# set NA to 0
df[is.na(df)] <- 0
nsv <- nearZeroVar(df,saveMetrics=TRUE)
# remove zero columns
df <- df[,-which(names(df) %in% c(rownames(nsv[nsv$zeroVar,])))]
return(df)
}
set.seed(2172015)
training <- read.csv("pml-training.csv")
testing <- read.csv("pml-testing.csv")
training<-preprocessData(training)
testing[is.na(testing)] <- 0
testing <- testing[, which(names(testing) %in% names(training))]
fitControl <- trainControl(method="repeatedcv",
number=5,
repeats=1,
verboseIter=TRUE)
gbm_model<-train(classe~.,data=training,method="gbm",
trControl=fitControl,
preProcess=c("scale", "pca"),
verbose=FALSE)
testGbm<- predict(gbm_model, testing)
testGbm
# preprocess, center, scale, pca
preProc <- preProcess(training[,-124], method="pca")
trainPC <- predict(preProc,training[,-124])
library(rpart)
modelFit <- rpart(training$classe ~ .,method="class",data=trainPC)
testPC <- predict(preProc,testing)
testRpart<- predict(modelFit, testPC, type="vector")
testRpart
model.all <- lm(classes ~ ., data = training)
# n is the size of the data frame
n <- nrow(training)
# Stepwise Algorithm, result is the final model
model.new <- step(model.all, direction = "both", k = log(n), trace=1)
summary(model.new)
training <- read.csv("pml-training.csv")
testing <- read.csv("pml-testing.csv")
training<-preprocessData(training)
model.all <- lm(classes ~ ., data = training)
model.all <- lm(classe ~ ., data = training)
testing
rf_model <- train(classe~.,data=training,method="rf",
trControl=fitControl,
preProcess=c("scale", "pca"))
testRf<- predict(rf_model, testing)
testRf
testing[is.na(testing)] <- 0
testing <- testing[, which(names(testing) %in% names(training))]
testRf<- predict(rf_model, testing)
testRf
rm(list=ls())
library(caret)
preprocessData <- function(df) {
# remove columns with #DIV/0!
df <- df[,-which(names(df) %in% c("new_window","amplitude_yaw_belt", "amplitude_yaw_forearm","amplitude_yaw_dumbbell", "kurtosis_yaw_forearm", "kurtosis_yaw_belt", "skewness_yaw_belt", "kurtosis_yaw_dumbbell", "skewness_yaw_dumbbell", "skewness_yaw_forearm"))]
dropInd <- which(apply(df, 1, function(x) any(x=="#DIV/0!")))
df <- df[-dropInd,]
library(caret)
# convert factors to numeric
facs <- sapply(df[,-ncol(df)], is.factor)
df[ , facs] <- as.numeric(as.character(df[ , facs]))
# set NA to 0
df[is.na(df)] <- 0
nsv <- nearZeroVar(df,saveMetrics=TRUE)
# remove zero columns
df <- df[,-which(names(df) %in% c(rownames(nsv[nsv$zeroVar,])))]
return(df)
}
set.seed(2172015)
training <- read.csv("pml-training.csv")
testing <- read.csv("pml-testing.csv")
training<-preprocessData(training)
testing[is.na(testing)] <- 0
testing <- testing[, which(names(testing) %in% names(training))]
fitControl <- trainControl(method="cv",
number=3,
repeats=1,
verboseIter=TRUE)
gbm_model<-train(classe~.,data=training,method="gbm",
trControl=fitControl,
preProcess=c("scale", "pca"),
verbose=FALSE)
testGbm<- predict(gbm_model, testing)
testGbm
# preprocess, center, scale, pca
preProc <- preProcess(training[,-124], method="pca")
trainPC <- predict(preProc,training[,-124])
# RPart
library(rpart)
modelFit <- rpart(training$classe ~ .,method="class",data=trainPC)
testPC <- predict(preProc,testing)
testRpart<- predict(modelFit, testPC, type="vector")
testRpart
# Random Forest
rf_model <- train(classe~.,data=training,method="rf",
trControl=fitControl,
preProcess=c("scale", "pca"))
testRf<- predict(rf_model, testing)
testRf
library(caret)
preprocessData <- function(df) {
# remove columns with #DIV/0!
df <- df[,-which(names(df) %in% c("new_window","amplitude_yaw_belt", "amplitude_yaw_forearm","amplitude_yaw_dumbbell", "kurtosis_yaw_forearm", "kurtosis_yaw_belt", "skewness_yaw_belt", "kurtosis_yaw_dumbbell", "skewness_yaw_dumbbell", "skewness_yaw_forearm"))]
dropInd <- which(apply(df, 1, function(x) any(x=="#DIV/0!")))
df <- df[-dropInd,]
library(caret)
# convert factors to numeric
facs <- sapply(df[,-ncol(df)], is.factor)
df[ , facs] <- as.numeric(as.character(df[ , facs]))
# set NA to 0
df[is.na(df)] <- 0
nsv <- nearZeroVar(df,saveMetrics=TRUE)
# remove zero columns
df <- df[,-which(names(df) %in% c(rownames(nsv[nsv$zeroVar,])))]
return(df)
}
set.seed(2172015)
training <- read.csv("pml-training.csv")
testing <- read.csv("pml-testing.csv")
training<-preprocessData(training)
testing[is.na(testing)] <- 0
testing <- testing[, which(names(testing) %in% names(training))]
fitControl <- trainControl(method="cv",
number=2,
repeats=1,
verboseIter=TRUE)
gbm_model<-train(classe~.,data=training,method="gbm",
trControl=fitControl,
preProcess=c("scale", "pca"),
verbose=FALSE)
testGbm<- predict(gbm_model, testing)
testGbm
# preprocess, center, scale, pca
preProc <- preProcess(training[,-124], method="pca")
trainPC <- predict(preProc,training[,-124])
# RPart
library(rpart)
modelFit <- rpart(training$classe ~ .,method="class",data=trainPC)
testPC <- predict(preProc,testing)
testRpart<- predict(modelFit, testPC, type="vector")
testRpart
# Random Forest
rf_model <- train(classe~.,data=training,method="rf",
trControl=fitControl,
preProcess=c("scale", "pca"))
testRf<- predict(rf_model, testing)
testRf
library(caret)
preprocessData <- function(df) {
# remove columns with #DIV/0!
df <- df[,-which(names(df) %in% c("new_window","amplitude_yaw_belt", "amplitude_yaw_forearm","amplitude_yaw_dumbbell", "kurtosis_yaw_forearm", "kurtosis_yaw_belt", "skewness_yaw_belt", "kurtosis_yaw_dumbbell", "skewness_yaw_dumbbell", "skewness_yaw_forearm"))]
dropInd <- which(apply(df, 1, function(x) any(x=="#DIV/0!")))
df <- df[-dropInd,]
library(caret)
# convert factors to numeric
facs <- sapply(df[,-ncol(df)], is.factor)
df[ , facs] <- as.numeric(as.character(df[ , facs]))
# set NA to 0
df[is.na(df)] <- 0
nsv <- nearZeroVar(df,saveMetrics=TRUE)
# remove zero columns
df <- df[,-which(names(df) %in% c(rownames(nsv[nsv$zeroVar,])))]
return(df)
}
set.seed(2172015)
training <- read.csv("pml-training.csv")
testing <- read.csv("pml-testing.csv")
training<-preprocessData(training)
testing[is.na(testing)] <- 0
testing <- testing[, which(names(testing) %in% names(training))]
fitControl <- trainControl(method="cv",
number=2,
repeats=1,
verboseIter=TRUE)
gbm_model<-train(classe~.,data=training,method="gbm",
trControl=fitControl,
preProcess=c("scale", "pca"),
verbose=FALSE)
gbm_model
testGbm<- predict(gbm_model, testing)
testGbm
# preprocess, center, scale, pca
preProc <- preProcess(training[,-124], method="pca")
trainPC <- predict(preProc,training[,-124])
# RPart
library(rpart)
modelFit <- rpart(training$classe ~ .,method="class",data=trainPC)
modelFit
testPC <- predict(preProc,testing)
testRpart<- predict(modelFit, testPC, type="vector")
testRpart
# Random Forest
rf_model <- train(classe~.,data=training,method="rf",
trControl=fitControl,
preProcess=c("scale", "pca"))
rf_model
testRf<- predict(rf_model, testing)
testRf
